[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBuilding a Daily Digest Bot: Automating Substack to PDF\n\n\n\n\n\n\nR\n\n\nAutomation\n\n\nDocker\n\n\n\nHow I built an automated pipeline in R to curate, format, and deliver my favorite Substack newsletters as a daily printable newspaper.\n\n\n\n\n\nFeb 20, 2026\n\n\nCarson Slater\n\n\n\n\n\n\n\n\n\n\n\n\nComing Soon\n\n\n\n\n\n\nnews\n\n\n\nStay tuned for more content!\n\n\n\n\n\nNov 23, 2025\n\n\nCarson Slater\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carson Slater",
    "section": "",
    "text": "I am a third-year graduate student in the Department of Statistical Science at Baylor University. I moved to Waco after completing my undergraduate studies at Wheaton College, where I played baseball for the Thunder. I proudly hail from Phoenix and will spend every second I can visiting northern Arizona.\nGenerally speaking, you can find me at Grace Church Waco on a given Sunday morning, and I’m always trying to find a way to go back to the Czech Republic to work with Křesťanská Akademie Mladých again.\nCurrently, I am learning many new statistical methods that will enable me to build predictive models, understand stochastic phenomena, and discover associations in our world."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Carson Slater",
    "section": "",
    "text": "I am a third-year graduate student in the Department of Statistical Science at Baylor University. I moved to Waco after completing my undergraduate studies at Wheaton College, where I played baseball for the Thunder. I proudly hail from Phoenix and will spend every second I can visiting northern Arizona.\nGenerally speaking, you can find me at Grace Church Waco on a given Sunday morning, and I’m always trying to find a way to go back to the Czech Republic to work with Křesťanská Akademie Mladých again.\nCurrently, I am learning many new statistical methods that will enable me to build predictive models, understand stochastic phenomena, and discover associations in our world."
  },
  {
    "objectID": "posts/welcome-post/index.html",
    "href": "posts/welcome-post/index.html",
    "title": "Coming Soon",
    "section": "",
    "text": "In the meantime, enjoy this cool 4D graphic I made a few years ago for a class I was in…"
  },
  {
    "objectID": "posts/welcome-post/index.html#visualizing-complex-functions-with-domain-coloring",
    "href": "posts/welcome-post/index.html#visualizing-complex-functions-with-domain-coloring",
    "title": "Coming Soon",
    "section": "Visualizing Complex Functions with Domain Coloring",
    "text": "Visualizing Complex Functions with Domain Coloring\nOne of the challenges of working with complex-valued functions is that they’re difficult to visualize. For a real-valued function \\(f: \\mathbb{R} \\to \\mathbb{R}\\), we can easily plot it on a 2D graph. But for a complex function \\(f: \\mathbb{C} \\to \\mathbb{C}\\), both the input and output are two-dimensional, requiring four dimensions total to represent fully.\nDomain coloring provides an elegant solution: we plot the domain (the \\(xy\\)-plane representing \\(\\mathbb{C} \\cong \\mathbb{R}^2\\)) and use color to encode information about the output \\(f(z)\\). Specifically:\n\nHue represents the argument (angle) of \\(f(z)\\): \\(\\text{Arg}(f(z)) \\in (-\\pi, \\pi]\\)\nLightness represents the modulus (magnitude) of \\(f(z)\\): \\(|f(z)|\\)\n\nBelow, I’ve visualized the function:\n\\[f(z) = \\frac{z^2 + 1}{z^2 - 1}\\]\nThis function has two roots at \\(z = \\pm i\\) (where the numerator equals zero) and two poles at \\(z = \\pm 1\\) (where the denominator equals zero). In the visualization, you can identify these special points by observing where colors converge or diverge dramatically.\n\n\nCode\n# functions\nmodulus &lt;- function(z) Re(sqrt(z * Conj(z)))\nf &lt;- function(z) (z^2 + 1)/(z^2 - 1)\nrad2deg &lt;- function(rad, rotate = 0) ((rad + rotate) * 360/(2*pi)) %% 360\n\n# definitions\n# boundaries\nL &lt;- -2\nU &lt;- 2\n# making mesh\ndomain &lt;- seq(L, U, length.out = 1001)\n\nmesh &lt;- expand.grid(domain, domain) |&gt; \n  mutate(\"x\" = Var1, \n         \"y\" = Var2) |&gt; \n  select(x, y)\n\n# color mapping components\nz &lt;- complex(real = mesh$x, imaginary = mesh$y)\n\nfz &lt;- z |&gt; f()\n\nmod_fz &lt;- fz |&gt; modulus()\n\n# I ended using the second one from the Wikipedia page \n# and I added a constant to shift the color, as well as\n# a scale of 100 (a = 0.4 too)\n\na &lt;- 0.4\nell1 &lt;- (2/pi)*atan(mod_fz) + 65\nell2 &lt;- (mod_fz^a/(mod_fz^a + 1))/10 + 65 \nell3 &lt;- 100*mod_fz^a/(mod_fz^a + 1) + 25 # used this one \n\n# used saturation of 80\nmy_colors &lt;- Arg(fz) |&gt;\n  rad2deg() |&gt;\n  cbind(80, ell3) |&gt; \n  farver::encode_colour(from = \"hsl\")\n\ndf &lt;- cbind(mesh, my_colors) |&gt;\n  as.data.frame()\n\n# plot replica attempt\ndf |&gt;\n  ggplot(aes(x, y)) +\n  geom_raster(aes(fill = my_colors)) + \n  labs(x = \"Re(z)\", y = \"Im(z)\") +\n  scale_fill_identity() +\n  coord_equal(xlim = c(-2,2), ylim = c(-2,2)) +\n  theme(\n    panel.background = element_rect(fill = \"transparent\", color = NA),\n    plot.background = element_rect(fill = \"transparent\", color = NA),\n    panel.grid = element_blank(),\n    axis.text = element_text(color = \"white\"),\n    axis.title = element_text(color = \"white\")\n  )"
  },
  {
    "objectID": "posts/daily-digest-bot/index.html#why-i-built-this",
    "href": "posts/daily-digest-bot/index.html#why-i-built-this",
    "title": "Building a Daily Digest Bot: Automating Substack to PDF",
    "section": "Why I Built This",
    "text": "Why I Built This\nLet’s face it. The digital world is growing more complex and attention-seeking. That’s why I love places like Substack. It’s a corner of the internet where longer-form content continues to live on, but also a place with the flexibilty to curate your reading to feature writers you enjoy. It usually takes at most 10 seconds (often less) to watch a reel, but for a Substack article, you have to dig your heels in and enter their conversation. Rarely have I read a Substack article from a writer I enjoy and thought to myself afterwards, “I think I just caved to brain rot.”\nLately, I’ve noticed a shift though. As I subscribe to more and more writers, my feed also propogates with more and more ‘notes,’ most of which are suggested from people whom I do not subscribe to. To put it bluntly, I’ve pretty much determined that notes on Substack are eerily similar to tweets, and I honestly hate that. It feels as though Substack is becoming a writing-oriented version of Twitter (now X).\nInstead of yielding to the developers of Substack and accepting this change on the platform, or evading Substack entirely, I have found a way to indulge myself in (free) articles by the writers I enjoy, without even logging onto the app, or going on substack.com. As a matter of fact, I’ve deleted the app from my phone (I have no app store and can’t download apps without a password my wife and friends have). My alternative to Substack is utilizing a print version of the app, and going analog with Substack.\nThis is why I built my daily digest bot, which fetches my feed (of articles, and not notes) from the last 24 hours and compiles them into a PDF at 6:30am every morning. I share this post to help others with technical background escape the temptation to doomscroll our beloved Substack."
  },
  {
    "objectID": "posts/daily-digest-bot/index.html#how-i-did-it-the-technical-stack",
    "href": "posts/daily-digest-bot/index.html#how-i-did-it-the-technical-stack",
    "title": "Building a Daily Digest Bot: Automating Substack to PDF",
    "section": "How I Did It: The Technical Stack",
    "text": "How I Did It: The Technical Stack\nBuilding the daily digest was an exercise in stringing together a few very cool tools. The entire workflow is orchestrated in R, connecting a custom feed parser, a third-party print service, and a containerized cloud deployment into a single automated pipeline.\nI wanted a hands-off system that would automatically figure out what I’m subscribed to, find the newest articles, format them elegantly, and email them to me. (If you want to jump straight to the full source code, you can find the complete project repository on my GitHub here: https://github.com/carsonslater/dailydigest\nHere is a breakdown of the four main pillars of the project:\n\n1. Extracting the Subscriptions (The OPML Export)\nThe first challenge was getting a dynamic list of every publication I’m subscribed to. If you are not familiar, most blogs and newsletters have an RSS feed—a standardized way to distribute content behind the scenes. However, Substack doesn’t offer a clean, native way to export your reading list as a compilation of these RSS feeds.\nTo solve this, I used a brilliant JavaScript snippet by Les Orchard, which runs directly in the browser console. It scrapes your Substack reader interface and outputs a standard OPML file.\nOPML (Outline Processor Markup Language) is essentially just a structured address book for RSS feeds. It contains the exact htmlUrl for all of your subscriptions in one tidy XML document. By keeping this OPML file in my project directory (substack_subscriptions.opml), the R script can just parse the file to get the URLs. This means I can add new subscriptions seamlessly in the future by just regenerating the OPML file, without ever touching the core R code.\n# Snippet from daily_digest.R\nlibrary(xml2)\nopml_path &lt;- \"substack_subscriptions.opml\"\nopml &lt;- read_xml(opml_path)\nopml_links &lt;- xml_find_all(opml, \"//outline[@type='rss']\") %&gt;%\n  xml_attr(\"htmlUrl\") %&gt;%\n  keep(~ !is.na(.x))\n\n\n2. Parsing Feeds & Filtering in R\nOnce I had the list of Substack URLs from the OPML file, the script iterates through them to check the corresponding RSS endpoints (/feed).\nUsing the xml2 and lubridate packages in R, the script reaches out to the RSS endpoints for every publication and downloads the raw data of the feed. It then extracts the individual articles (stored as &lt;item&gt; blocks) and filters the results based on the published date (&lt;pubDate&gt;). Instead of downloading everything, it keeps only the posts that have been published within the last 24 hours. This ensures that I only pull the newest content, creating a highly curated, manageable queue of fresh article URLs that are ready for printing.\nrss &lt;- read_xml(rss_url)\nitems &lt;- xml_find_all(rss, \".//item\")\n\nfor (item in items) {\n  pub_date_str &lt;- xml_text(xml_find_first(item, \".//pubDate\"))\n  pub_date &lt;- parse_date_time(pub_date_str, \"a d b Y H M S\")\n  \n  if (!is.na(pub_date) && pub_date &gt;= threshold_date) {\n    post_link &lt;- xml_text(xml_find_first(item, \".//link\"))\n    all_post_urls &lt;- c(all_post_urls, post_link)\n  }\n}\n\n\n3. Rendering via Substackprint.com\nConverting standard HTML into a clean, printable PDF is notoriously tricky—especially when dealing with massive, oversized images and weird formatting across various newsletters. Initially, I experimented with complex typesetting engines locally, but that resulted in massive styling headaches.\nInstead, I decided to lean on Substackprint.com. It’s a fantastic, specialized tool designed specifically to strip away the digital clutter from Substack posts and elegantly compile them into a beautiful, newspaper-style PDF.\nTo automate this without manually copying and pasting URLs every morning, my script passes the filtered lists of Substack URLs directly to Substackprint’s generator using the {chromote} package.\n{chromote} acts as a headless browser—meaning it can run Google Chrome in the background without actually popping up a window on your screen, controlled entirely by code. The script uses this invisible browser to load Substackprint, click the “Multi-Publication mode” button, inject my curated list of URLs into the text box, and click the “Go Analog!” submit button.\nOnce Substackprint finishes rendering the newspaper on the page, the headless browser captures the final layout and uses Chrome’s native built-in PDF generator to save the gorgeous digest directly to my local drive.\n# Snippet from helpers/pdf_generator.R\nlibrary(chromote)\nb &lt;- ChromoteSession$new()\n\n# Navigate to tool and inject URLs\nb$Page$navigate(\"https://substackprint.com/\")\nb$Runtime$evaluate(\"document.getElementById('pub-mode-multi').click();\")\n\nurls_string &lt;- paste(post_urls, collapse = \"\\n\")\nb$Runtime$evaluate(glue(\n  \"document.getElementById('substack-urls').value = `{urls_string}`;\"\n))\n\n# Generate and save PDF\nb$Runtime$evaluate(\"document.querySelector('#substack-form button[type=\\\"submit\\\"]').click();\")\n# ... polling to wait for generation ...\npdf_data &lt;- b$Page$printToPDF(printBackground = TRUE, marginTop = 0, marginBottom = 0)\nwriteBin(jsonlite::base64_dec(pdf_data$data), output_path)\n\n\n4. Automated Email Delivery\nGenerating a beautiful PDF is great, but to make this a true “daily digest,” I needed it to arrive in my inbox automatically while I was waking up.\nFor email delivery, I used the {blastula} package in R. {blastula} makes it incredibly easy to compose an email body, attach the compiled PDF, and securely dispatch it via SMTP.\nSMTP (Simple Mail Transfer Protocol) is the standard protocol that email servers use to send messages across the internet. By plugging in my email provider’s SMTP credentials directly into the script, R can securely send the email attachment just as if I were logging into Gmail myself.\n# Snippet from helpers/email_sender.R\nlibrary(blastula)\n\nemail_obj &lt;- compose_email(\n  body = md(\"Here is your favorite Substack writer's latest work...\"),\n  footer = md(\"Generated via substackprint.com and R automation.\")\n)\nemail_obj &lt;- add_attachment(email_obj, file = pdf_path)\n\nsmtp_send(\n  email = email_obj,\n  from = config$email$from,\n  to = config$email$to,\n  credentials = creds_envvar(...)\n)\n\n\n5. Containerization and Deployment\nA daily digest isn’t very useful if I have to run the R script manually every morning. The final piece of the puzzle was scheduling this automation to run autonomously.\nIf I simply moved my code to another machine, it might fail because that hardware lacks the exact version of packages or software I had on my personal laptop. To solve this, I packed the entire project—the R dependencies, the OPML file, and the core scripts—into a Docker container. You can think of a Docker container as a mini, virtualized computer that wraps up the code and all its exact dependencies so it runs identically anywhere.\nBecause {chromote} requires a headless browser to generate the PDF via Substackprint, the container cannot just run R; it must also have Google Chrome fully installed. When I built the Dockerfile, the trickiest part was ensuring the system dependencies were perfectly configured to run Chrome invisibly without sandboxing errors.\nHere is the crux of the Dockerfile that makes this all possible:\n# Start with a standard base R image setup\nFROM rocker/tidyverse:latest\n\n# Install Linux system dependencies for `{blastula}` (crypto, SSL) and `{xml2}`\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    wget gnupg libxml2-dev libssl-dev libcurl4-openssl-dev \\\n    zlib1g-dev libfontconfig1 libxt6 && rm -rf /var/lib/apt/lists/*\n\n# Download and install Google Chrome explicitly for `{chromote}` to use\nRUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \\\n    && echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ debian main\" &gt; /etc/apt/sources.list.d/google-chrome.list \\\n    && apt-get update && apt-get install -y --no-install-recommends \\\n    google-chrome-stable && rm -rf /var/lib/apt/lists/*\n\n# Copy project files and install target R packages via setup.R\nWORKDIR /app\nCOPY . .\nRUN Rscript setup.R\n\n# Crucial Environment Variables: Tell `{chromote}` exactly where to find Chrome\n# and disable the sandbox so it can execute smoothly inside a Docker container\nENV CHROMOTE_CHROME=/usr/bin/google-chrome-stable\nENV CHROMOTE_CHROME_ARGS=\"--no-sandbox --disable-dev-shm-usage --headless\"\n\n# Point the container entry command to the target script\nCMD [\"Rscript\", \"daily_digest.R\"]\nThe real “aha!” moment is the bottom section with ENV CHROMOTE_CHROME_ARGS. Because Docker containers isolate their processes, running an entire browser inside them can lead to bizarre memory allocation issues and permission errors. Flags like --no-sandbox and --disable-dev-shm-usage are critical requirements to keep the invisible browser from crashing during the heavy lifting of the PDF generation step.\nFor the actual deployment, I repurposed a spare, headless Mac Mini provided by my job. Rather than running a persistent daemon inside the container, I let the host machine handle the scheduling. I set up the Mac Mini’s local cron tab to run a simple shell task at 6:30 AM every morning that executes docker run on my image. The container spins up, does its job, and gracefully exits. To ensure the Mac Mini doesn’t inadvertently go to sleep and skip the cron job, I keep it perpetually awake using the macOS caffeinate terminal command.\nNow, when the morning alarm finally goes off, the Mac Mini spins up the lightweight container, Chrome renders the PDF via Substackprint in the background, and {blastula} shoots the final product straight to my email inbox. It’s completely hands-off—allowing me to wake up, check my email, and have a beautifully formatted, highly curated newspaper waiting for me!\nI hope you enjoyed this post. If you’re interested in contributing, you can open up a pull request on my GitHub here: https://github.com/carsonslater/dailydigest, or you can send me an email."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Below are some of the projects I have worked on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssessing the Effects of Surface-Stabilized Zero-Valent Iron Nanoparticles\n\n\n\n\n\n\nNanoparticle Bacterial Inhibition\n\n\nOrdinal Regression\n\n\nGeneralized Linear Mixed Models\n\n\n\nThis paper employs statistical methods to do pairwise comparisons when ANOVA assumptions are egregiously violated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssessing Interference with Regression Analysis Techniques\n\n\n\n\n\n\nDynamic Spectrum Access\n\n\nGeneralized Additive Models\n\n\n\nThis work proposes using regression analysis to predict aggregate interference in next-generation Dynamic Spectrum Access (DSA) systems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMo(Wa)²TER Introduction to Data Science Course Material\n\n\n\n\n\n\nData Science\n\n\nR/RStudio\n\n\nWastewater Treatment\n\n\n\nWe used problems in water and wastewater treatment to create a data science course.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCobalt Theme for Positron\n\n\n\n\n\n\nPositron\n\n\n\nA dark theme based on the Cobalt RStudio theme for Positron (and VS Code).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngiftR\n\n\n\n\n\n\nR Package\n\n\nSatire\n\n\n\nA white elephant gift exchange facilitator.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "carson_slater1@baylor.edu\n carsonslater\n carsonslateratbaylor\n carson.sl8r\n\n\n\nPh.D., Statistical Science\nBaylor University\n2024–Pending\nM.S., Statistical Science\nBaylor University\n2023–2024\nB.S., Applied Math / B.A., Econ\nWheaton College\n2019–2023\nEdman Presidential Scholarship\n\n\n\nTools\nDocker, RStudio, Git/GitHub, reveal.js, Positron\nLanguages\nR, JAGS/Stan, Python, C, SAS, Stata, SQL, zsh, LaTeX, JS, CSS, HTML\n\n\n\nVarsity Baseball Player\nWheaton College\n2019–2022\nChristian Character Award\nValley Christian HS\n2019\n\n\n\n\n\nMotivated graduate student seeking consulting opportunities. Aspiring applied statistician who loves to use statistical methods to empower people with insights for good stewardship.\n\n\n\n\n\nBaylor University | Jan 2024–Present\n\nConducts research with Dr. Mandy Hering with collaborators in urban water treatment and infrastructure.\n\n\n\n\nBaylor University | Aug 2023–Dec 2024\n\nCollaborated in the Statistical Consulting Center on interdisciplinary research.\nTA for Computational Statistics (STA 4373/5373/6375).\nLed four supplemental instruction sessions weekly for Introduction to Statistical Methods (STA 2381).\n\n\n\n\nWheaton Academy Center for Lifelong Learning | Jan 2023–Mar 2023\n\nTaught and reviewed mathematics with students ranging from Algebra I to AP Statistics.\n\n\n\n\nWheaton College | Aug 2022–May 2023\n\nTA for Applied Machine Learning (MATH 465) and Probability Theory (MATH 363).\nGraded weekly assignments and hosted help sessions.\n\n\n\n\nDuke/NCSU Summer Institute for Biostatistics | Jun 2022–Jul 2022\n\nParticipated in biostatistics coursework at Duke Clinical Research Institute and NC State.\nCompleted a hackathon on myocardial infarction modeling. GitHub\n\n\n\n\n\n\n\nPaper\n\nUsed ordinal regression and generalized linear models to understand relationships between nanoparticle coatings and bacterial inhibition.\n\n\n\n\nPaper\n\nUsed generalized additive models to predict pairwise interference in a Dynamic Spectrum Access context.\nPresented at the 2024 IEEE Texas Symposium on Wireless and Microwave Circuits and Systems (WMCS).\n\n\n\n\n\n\n\nJun 2023–Aug 2023\n\nBuilt a blog to keep supporters updated on mission work.\nTaught English and sports in schools and at camps; supported local CB churches.\nTrained incoming short-term mission teams."
  },
  {
    "objectID": "cv.html#contact",
    "href": "cv.html#contact",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "carson_slater1@baylor.edu\n carsonslater\n carsonslateratbaylor\n carson.sl8r"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Ph.D., Statistical Science\nBaylor University\n2024–Pending\nM.S., Statistical Science\nBaylor University\n2023–2024\nB.S., Applied Math / B.A., Econ\nWheaton College\n2019–2023\nEdman Presidential Scholarship"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Tools\nDocker, RStudio, Git/GitHub, reveal.js, Positron\nLanguages\nR, JAGS/Stan, Python, C, SAS, Stata, SQL, zsh, LaTeX, JS, CSS, HTML"
  },
  {
    "objectID": "cv.html#misc.",
    "href": "cv.html#misc.",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Varsity Baseball Player\nWheaton College\n2019–2022\nChristian Character Award\nValley Christian HS\n2019"
  },
  {
    "objectID": "cv.html#summary",
    "href": "cv.html#summary",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Motivated graduate student seeking consulting opportunities. Aspiring applied statistician who loves to use statistical methods to empower people with insights for good stewardship."
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Baylor University | Jan 2024–Present\n\nConducts research with Dr. Mandy Hering with collaborators in urban water treatment and infrastructure.\n\n\n\n\nBaylor University | Aug 2023–Dec 2024\n\nCollaborated in the Statistical Consulting Center on interdisciplinary research.\nTA for Computational Statistics (STA 4373/5373/6375).\nLed four supplemental instruction sessions weekly for Introduction to Statistical Methods (STA 2381).\n\n\n\n\nWheaton Academy Center for Lifelong Learning | Jan 2023–Mar 2023\n\nTaught and reviewed mathematics with students ranging from Algebra I to AP Statistics.\n\n\n\n\nWheaton College | Aug 2022–May 2023\n\nTA for Applied Machine Learning (MATH 465) and Probability Theory (MATH 363).\nGraded weekly assignments and hosted help sessions.\n\n\n\n\nDuke/NCSU Summer Institute for Biostatistics | Jun 2022–Jul 2022\n\nParticipated in biostatistics coursework at Duke Clinical Research Institute and NC State.\nCompleted a hackathon on myocardial infarction modeling. GitHub"
  },
  {
    "objectID": "cv.html#papers",
    "href": "cv.html#papers",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Paper\n\nUsed ordinal regression and generalized linear models to understand relationships between nanoparticle coatings and bacterial inhibition.\n\n\n\n\nPaper\n\nUsed generalized additive models to predict pairwise interference in a Dynamic Spectrum Access context.\nPresented at the 2024 IEEE Texas Symposium on Wireless and Microwave Circuits and Systems (WMCS)."
  },
  {
    "objectID": "cv.html#volunteer",
    "href": "cv.html#volunteer",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Jun 2023–Aug 2023\n\nBuilt a blog to keep supporters updated on mission work.\nTaught English and sports in schools and at camps; supported local CB churches.\nTrained incoming short-term mission teams."
  }
]